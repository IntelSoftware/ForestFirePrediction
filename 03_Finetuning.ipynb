{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e8dd31e-d502-4f69-9614-90b3914e7e80",
   "metadata": {},
   "source": [
    "# Finetuning\n",
    "\n",
    "![asstes/FolderStructure.jpg](assets/FolderStructure.jpg)\n",
    "\n",
    "\n",
    "Due to the sort order 'Fire' comes before 'NoFire' so the class index is as follows:\n",
    "\n",
    "- 0: Fire\n",
    "- 1: NoFire\n",
    "\n",
    "While I have created a custom kernel for this class to access the GPU - there is a standard kernel Intel provides (without the support libraries such as pandas, and seabron, etc) called **pytorch_xpu** that I would recommend for quick testing of a PyTorch model.\n",
    "\n",
    "## Fintuning is awesome! \n",
    "\n",
    "![assets/fine_tuning.png](assets/fine_tuning.png)\n",
    "\n",
    "Finetuning allows me to train a small number of iterations of a small number of images to create a custom model quickly.  It does this by freezing most of the layers of a pretrianed model such as resnet, and allowing me to train just the very last layers.\n",
    "\n",
    "As a reuslt, this notebook can be run quite fast on our Intel(r) 4th generation Xeon(r) Scalable processors!  But it can allso be run even faster on out Intel(r) PVC GPU's\n",
    "\n",
    "We will be deomonstraing how to train against the new Intel(r) GPUS in this exercise - taking advantage of c key ingredient supplied by the Intel(R) Extensions for PyTorch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20937d85-1fc7-4204-9a52-58881711e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load fine_tune.py\n",
    "import os\n",
    "import pathlib\n",
    "import warnings\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "from typing import Tuple\n",
    "#import glob\n",
    "from PIL import Image\n",
    "from os.path import exists\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import intel_extension_for_pytorch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import seaborn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from batch_finder import optimum_batch_size\n",
    "from config import set_seed, device\n",
    "from data_loader import (\n",
    "    TRAIN_DIR,\n",
    "    VALID_DIR,\n",
    "    augment_and_save,\n",
    "    data_distribution,\n",
    "    imagenet_stats,\n",
    "    img_transforms,\n",
    "    plot_data_distribution,\n",
    "    show_data,\n",
    ")\n",
    "from metrics import Metrics\n",
    "from model import FireFinder\n",
    "from trainer import Trainer\n",
    "from lr_finder import LearningRateFinder\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "\n",
    "def create_dataloader(\n",
    "    directory: str, batch_size: int, shuffle: bool = False, transform=None\n",
    ") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Create a DataLoader from a directory of images.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Directory containing images.\n",
    "        batch_size (int): Batch size for the DataLoader.\n",
    "        shuffle (bool, optional): Whether to shuffle the data. Defaults to False.\n",
    "        transform ([type], optional): Transformations to apply to the images. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: DataLoader with images from the directory.\n",
    "    \"\"\"\n",
    "    data = datasets.ImageFolder(directory, transform=transform)\n",
    "    return DataLoader(data, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "\n",
    "def setup_dataloaders(config: dict) -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Setup train and validation DataLoaders.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Configuration dictionary containing batch_size.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[DataLoader, DataLoader]: A tuple containing train and validation dataloaders.\n",
    "    \"\"\"\n",
    "    return create_dataloader(\n",
    "        TRAIN_DIR, config[\"batch_size\"], shuffle=True, transform=img_transforms[\"train\"]\n",
    "    ), create_dataloader(\n",
    "        VALID_DIR, config[\"batch_size\"], transform=img_transforms[\"valid\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def find_lr(model: FireFinder, optimizer: optim.Adam, dataloader: DataLoader) -> float:\n",
    "    \"\"\"\n",
    "    Find best learning rate using Learning Rate Finder.\n",
    "\n",
    "    Args:\n",
    "        model (FireFinder): FireFinder model.\n",
    "        optimizer (optim.Adam): Adam optimizer.\n",
    "        dataloader (DataLoader): DataLoader with training data.\n",
    "\n",
    "    Returns:\n",
    "        float: Best learning rate.\n",
    "    \"\"\"\n",
    "    lr_finder = LearningRateFinder(model, optimizer, device)\n",
    "    best_lr = lr_finder.lr_range_test(dataloader, start_lr=1e-2, end_lr=1e-5)\n",
    "    return best_lr\n",
    "\n",
    "\n",
    "def train(model: FireFinder, trainer: Trainer, config: dict):\n",
    "    \"\"\"\n",
    "    Train a FireFinder model.\n",
    "\n",
    "    Args:\n",
    "        model (FireFinder): FireFinder model.\n",
    "        trainer (Trainer): Trainer to train the model.\n",
    "        config (dict): Configuration dictionary containing learning rate and batch size.\n",
    "    \"\"\"\n",
    "    train_dataloader, valid_dataloader = setup_dataloaders(config)\n",
    "    print(\"training data\")\n",
    "    plot_data_distribution(data_distribution(train_dataloader.dataset, TRAIN_DIR))\n",
    "    print(\"\\nvalidation data\")\n",
    "    plot_data_distribution(data_distribution(valid_dataloader.dataset, VALID_DIR))\n",
    "    print(f\"______________\")\n",
    "    start = time.time()\n",
    "    val_acc = trainer.fine_tune(train_dataloader, valid_dataloader)\n",
    "    model_save_path = f\"./models/model_acc_{val_acc}_device_{device}_lr_{trainer.lr}_epochs_{EPOCHS}.pt\"\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model_scripted = torch.jit.script(model)  # Export to TorchScript\n",
    "        model_scripted.save(f\"{model_save_path.replace('.pt','_jit.pt')}\")  # Jit Save\n",
    "\n",
    "    print(f\"Model saved to :{model_save_path}\")\n",
    "    print(f\"Time elapsed: {time.time() - start} seconds.\")\n",
    "    return (model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac78f28-1721-4bcf-b856-2b88564ff949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "EPOCHS = 5\n",
    "DROPOUT = .3\n",
    "# LR would be changed if we are using a LR finder\n",
    "LR = 2.14e-4\n",
    "#LR = 3.e-3\n",
    "TEST_DIR = 'data/shift/'\n",
    "BATCH_SIZE = 64 #128  # Default batch size\n",
    "\n",
    "aug_data = False\n",
    "find_batch = False\n",
    "find_lr_rate = False\n",
    "use_wandb = True\n",
    "use_ipex=True\n",
    "\n",
    "set_seed(42)\n",
    "print(f\"Train folder {TRAIN_DIR}\")\n",
    "print(f\"Validation folder {VALID_DIR}\")\n",
    "print(f\"Using epoch: {EPOCHS}\")\n",
    "print(f\"Using Dropout: {DROPOUT}\")\n",
    "\n",
    "batch_size = BATCH_SIZE\n",
    "\n",
    "if aug_data:\n",
    "    print(\"Augmenting training and validation datasets...\")\n",
    "    t1 = time.time()\n",
    "    augment_and_save(TRAIN_DIR)\n",
    "    augment_and_save(VALID_DIR)\n",
    "    print(f\"Done Augmenting in {time.time() - t1} seconds...\")\n",
    "\n",
    "model = FireFinder(simple=True, dropout=DROPOUT)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "if find_batch:\n",
    "    print(f\"Finding optimum batch size...\")\n",
    "    batch_size = optimum_batch_size(model, input_size=(3, 224, 224))\n",
    "print(f\"Using batch size: {batch_size}\")\n",
    "\n",
    "best_lr = LR\n",
    "if find_lr_rate:\n",
    "    print(\"Finding best init lr...\")\n",
    "    train_dataloader = create_dataloader(\n",
    "        TRAIN_DIR,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        transform=img_transforms[\"train\"],\n",
    "    )\n",
    "    best_lr = find_lr(model, optimizer, train_dataloader)\n",
    "    del model, optimizer\n",
    "    gc.collect()\n",
    "    if device == torch.device(\"xpu\"):\n",
    "        torch.xpu.empty_cache()\n",
    "print(f\"Using learning rate: {best_lr}\")\n",
    "\n",
    "model = FireFinder(simple=True, dropout=DROPOUT)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optim.Adam,\n",
    "    lr=best_lr,\n",
    "    epochs=EPOCHS,\n",
    "    device=device,\n",
    "    use_wandb=use_wandb,\n",
    "    use_ipex=use_ipex,\n",
    ")\n",
    "model_save_path = train(model, trainer, config={\"lr\": best_lr, \"batch_size\": batch_size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71100ac-96f8-4512-b090-56e314af9b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(InFldr, ScoreDict, model):\n",
    "    import torchvision.transforms as transforms\n",
    "    import torch.nn as nn\n",
    "    import glob \n",
    "    model.eval()\n",
    "\n",
    "    scores = []\n",
    "    fns = []\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((1024, 1024)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(*imagenet_stats),\n",
    "    ])\n",
    "    \n",
    "    for fn in glob.glob(TEST_DIR+'*.png'):\n",
    "        img = Image.open(fn)\n",
    "        img_tensor = transform(img).reshape(1, 3, 1024, 1024)\n",
    "        score = 0\n",
    "        modelCPU = model.to(\"cpu\")\n",
    "        score = np.argmax(torch.softmax( modelCPU(img_tensor).detach(), dim =1).numpy().squeeze())\n",
    "        fns.append(fn)\n",
    "        scores.append(ScoreDict[score])\n",
    "    return (scores, fns)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    model_scripted = torch.jit.script(model.to(\"cpu\"))  # Export to TorchScript\n",
    "    model_scripted.save(model_save_path)  # Jit Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e705b6c2-95e4-49bf-b1f4-77f27bb87be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path\n",
    "#resnet18_simpleFalse_IPEXTrue_Epochs12_dropout0.4_batch128\n",
    "#model_name = f\"resnet18_simple{simple}_IPEX{ipx}_Epochs{epochs}_dropout{dropout}_batch{batch_size}\"\n",
    "print(model_save_path)\n",
    "\n",
    "model_read = torch.jit.load(model_save_path)\n",
    "#model_read.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af91c4-d6b2-4e05-88a6-009004ce1130",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DIR =  'data/shift/val/Fire/'\n",
    "scoreDict = {0:'Fire', 1:'NoFire'}\n",
    "scores, fns = predict(TEST_DIR, scoreDict, model)\n",
    "for score, fn in zip(scores, fns):\n",
    "    print(f\"{score}\\t{fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e248d4c2-f5d4-4768-83c8-8aa21745fc6d",
   "metadata": {},
   "source": [
    "Notices and Disclaimers\n",
    "Intel technologies may require enabled hardware, software or service activation.\n",
    "\n",
    "No product or component can be absolutely secure.\n",
    "\n",
    "Your costs and results may vary.\n",
    "\n",
    "© Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b571e-78b8-4383-b32e-5add2317ddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DIR =  'data/shift/val/NoFire/'\n",
    "scoreDict = {0:'Fire', 1:'NoFire'}\n",
    "scores, fns = predict(TEST_DIR, scoreDict, model_read)\n",
    "for score, fn in zip(scores, fns):\n",
    "    print(f\"{score}\\t{fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ad28ff-510e-495b-86f0-05f5f2a97818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621bb714-a733-4166-acab-0f5b72e2f3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b031c18-af26-45be-be23-face89f5336b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d22115-1442-4e2b-91d8-3dcb7d200b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT2",
   "language": "python",
   "name": "pt2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
