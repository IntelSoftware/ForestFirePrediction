{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c26d0ec2-0fc6-4dfd-8f77-f10657db5e8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Stable Diffusion: Image to Image version\n",
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"assets/m_3912120_ne_10_h_20160713_day_vegetation_dull_with_little_saturation_0.png\", width=\"800\">\n",
    "<figcaption align = \"center\"> Figure 1. A synthetic image generated with this Table Diffusion* code.</figcaption>\n",
    "</figure>\n",
    "\n",
    "<br></br>\n",
    "\n",
    "<figure>\n",
    "<img src=\"assets/stable diffusion_image_to_image.png\", width=\"800\">\n",
    "<figcaption align = \"center\"> Figure 2. A closer look at the architecture diagram of Stable Diffusion Components </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Overview\n",
    "\n",
    "I use Stable Diffusion to synthesize aerial photos for a number of reasons:\n",
    "\n",
    "1) Real images that are free to download are time consuming to obtain.\n",
    "2) Real images that are easy to obtain via nice API's, are typically NOT free.\n",
    "3) Time constraints limit the number I am willing to download.\n",
    "4) Distribution of real data to attendees is problematic from a usage license stand point.\n",
    "5) Synthetic images can be generated in bulk easily\n",
    "6) Ooptimizing Stable Diffusion pipeline using the Intel(R) Extensions for PyTorch*\n",
    "\n",
    "   \n",
    "Stable Diffusion is a technique that uses a special kind of diffusion model developed by the CompVis group at LMU Munich. This model helps improve images by reducing noise. It has three main parts: a variational autoencoder (VAE) that compresses the image, a U-Net that removes noise, and an optional text encoder. The VAE makes the image smaller and more meaningful, while the U-Net removes noise. The process involves applying Gaussian noise and then removing it. The VAE decoder completes the process by turning the compressed image back into a clear picture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102514d5-f74d-4216-a8ff-3f786ce7c8b7",
   "metadata": {},
   "source": [
    "# What can it do?\n",
    "\n",
    "<figure>\n",
    "<img src=\"assets/X-Y_plot_of_algorithmically-generated_AI_art_of_European-style_castle_in_Japan_demonstrating_DDIM_diffusion_steps.png\", width=\"600\">\n",
    "<figcaption align = \"center\"> Figure 2. Image courtesy of Benlisquare at wikipedia: Stable Diffusion (File:X-Y plot of algorithmically-generated AI art of European-style castle in Japan demonstrating DDIM diffusion steps.png) </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e829484-faeb-4c27-8cce-064c9135aa9e",
   "metadata": {},
   "source": [
    "# Stable Diffusion for Synthetic Aerial Photos\n",
    "\n",
    "In this project we downloaded 10 images from US government sources for NAIP/DOQQ images with two classes in mind - photos which will be marred in the future by forest fires, and photos which will remain un-phased by forest fires in the next two years (referencing 2016/2017).\n",
    "\n",
    "The code below was taken from Hugging face and optimized by our Intel team!\n",
    "\n",
    "It uses Intel(R) Extension for PyTorch* (IPEX) to allow us to target an IntelÂ® Data Center GPU Max Series GPU on the Intel(r) Developer Cloud Beta.\n",
    "\n",
    "The interesting take away here is that the Stable Diffusion model is not a single model but rather a pipeline of 4 standalone models and these optimization techniques get applied to all models in the pipeline!\n",
    "\n",
    "Significant speedups come from applying IPEX. Try it yourself!\n",
    "\n",
    "### Exercise 3: Modify the code below to use IPEX to optimize each stage of the SD pipeline\n",
    "\n",
    "The code from retrieved from Hugging Face has not had IPEX optimizations applied to each stage of the pipeline and will take ~ 17 minutes to run. To dramatically speed this up, complete Exercise 3!\n",
    "\n",
    "Find Exercise 3 below and un-comment the code or else replace with this code:\n",
    "```python\n",
    "        for attr in dir(pipeline):\n",
    "            if isinstance(getattr(pipeline, attr), nn.Module):\n",
    "                setattr(\n",
    "                    pipeline,\n",
    "                    attr,\n",
    "                    ipex.optimize(\n",
    "                        getattr(pipeline, attr).eval(),\n",
    "                        dtype=pipeline.text_encoder.dtype,\n",
    "                        inplace=True,\n",
    "                    ),\n",
    "                 )\n",
    "```\n",
    "\n",
    "### Exercise 4: Modify the text to generate custom images\n",
    "\n",
    "Add a few text descriptor sentences to help generate better images\n",
    "\n",
    "- Hint: try:\n",
    "\n",
    "```python\n",
    "fire_variations = [\n",
    "    \"vegetation dark green and brown low saturation image mountainous slopes early morning clear skies\",\n",
    "    \"vegetation dark green and brown low saturation image mountainous slopes late afternoon clear skies\",\n",
    "    \"vegetation dark green and brown low saturation image mountainous slopes mid-day no clouds\",\n",
    "    \"vegetation dark green and brown low saturation image mountainous slopes mid-day minimal light sirius cloud cover\",\n",
    "    \"vegetation dark green and brown low saturation image mountainous slopes smoky conditions\",\n",
    "    \"vegetation dark green and brown low saturation image mountainous slopes late afternoon minimal light sirius cloud cover\",\n",
    "]\n",
    "no_fire_variations = [\n",
    "    \"early morning with clear skies vegetation bright green vibrant saturation\",\n",
    "    \"no signs of fire vegetation bright green vibrant saturation\",\n",
    "    \"day vegetation bright green vibrant saturation\",\n",
    "    \"late afternoon with clear skies vegetation bright green vibrant saturation\",\n",
    "    \"mid-day with clear skies vegetation bright green vibrant saturation\",\n",
    "    \"with dense vegetation vegetation bright green vibrant saturation\",\n",
    "    \"with sparse vegetation vegetation bright green vibrant saturation\",\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdc4138-bfa4-4527-b611-c3345e2c7d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install  invisible-watermark > /dev/null\n",
    "# #!conda install -y --quiet --prefix {sys.prefix}  -c conda-forge \\\n",
    "# !pip install  \\\n",
    "#      accelerate==0.23.0 \\\n",
    "#      validators==0.22.0 \\\n",
    "#      diffusers==0.18.2 \\\n",
    "#     transformers==4.32.1 \\\n",
    "#     tensorboardX \\\n",
    "#     pillow \\\n",
    "#     ipywidgets \\\n",
    "#     intel-extension-for-pytorch\\\n",
    "#     ipython > /dev/null && echo \"Installation successful\" || echo \"Installation failed\"\n",
    "\n",
    "#standalone\n",
    "#!pip install  validators==0.22.0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8f99f6-61ea-4080-be0a-74610adfdf75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad1d3e5-05f8-4a15-a964-91c8c46e71db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import random\n",
    "import requests\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class Img2ImgModel:\n",
    "    \"\"\"\n",
    "    This class creates a model for transforming images based on given prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id_or_path: str,\n",
    "        device: str = \"xpu\",\n",
    "        torch_dtype: torch.dtype = torch.float16,\n",
    "        optimize: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the model with the specified parameters.\n",
    "\n",
    "        Args:\n",
    "            model_id_or_path (str): The ID or path of the pre-trained model.\n",
    "            device (str, optional): The device to run the model on. Defaults to \"xpu\".\n",
    "            torch_dtype (torch.dtype, optional): The data type to use for the model. Defaults to torch.float16.\n",
    "            optimize (bool, optional): Whether to optimize the model. Defaults to True.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.pipeline = self._load_pipeline(model_id_or_path, torch_dtype)\n",
    "        if optimize:\n",
    "            start_time = time.time()\n",
    "            print(\"Optimizing the model...\")\n",
    "            self.optimize_pipeline()\n",
    "            print(\n",
    "                \"Optimization completed in {:.2f} seconds.\".format(\n",
    "                    time.time() - start_time\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def _load_pipeline(\n",
    "        self, model_id_or_path: str, torch_dtype: torch.dtype\n",
    "    ) -> StableDiffusionImg2ImgPipeline:\n",
    "        \"\"\"\n",
    "        Load the pipeline for the model.\n",
    "\n",
    "        Args:\n",
    "            model_id_or_path (str): The ID or path of the pre-trained model.\n",
    "            torch_dtype (torch.dtype): The data type to use for the model.\n",
    "\n",
    "        Returns:\n",
    "            StableDiffusionImg2ImgPipeline: The loaded pipeline.\n",
    "        \"\"\"\n",
    "        print(\"Loading the model...\")\n",
    "        pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "            model_id_or_path, torch_dtype=torch_dtype\n",
    "        )\n",
    "        pipeline = pipeline.to(self.device)\n",
    "        print(\"Model loaded.\")\n",
    "        return pipeline\n",
    "\n",
    "\n",
    "    def _optimize_pipeline(self, pipeline: StableDiffusionImg2ImgPipeline) -> StableDiffusionImg2ImgPipeline:\n",
    "            \"\"\"\n",
    "            Optimize the pipeline of the model.\n",
    "    \n",
    "            Args:\n",
    "                pipeline (StableDiffusionImg2ImgPipeline): The pipeline to optimize.\n",
    "    \n",
    "            Returns:\n",
    "                StableDiffusionImg2ImgPipeline: The optimized pipeline.\n",
    "            \"\"\"\n",
    "    \n",
    "            for attr in dir(pipeline):\n",
    "                try:\n",
    "                    if isinstance(getattr(pipeline, attr), nn.Module):\n",
    "                        setattr(\n",
    "                            pipeline,\n",
    "                            attr,\n",
    "                            ipex.optimize(\n",
    "                                getattr(pipeline, attr).eval(),\n",
    "                                dtype=pipeline.text_encoder.dtype,\n",
    "                                inplace=True,\n",
    "                            ),\n",
    "                        )\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "            return pipeline\n",
    "    \n",
    "\n",
    "    def optimize_pipeline(self) -> None:\n",
    "        \"\"\"\n",
    "        Optimize the pipeline of the model.\n",
    "        \"\"\"\n",
    "        self.pipeline = self._optimize_pipeline(self.pipeline)\n",
    "\n",
    "    def get_image_from_url(self, url: str, path: str) -> Image.Image:\n",
    "        \"\"\"\n",
    "        Get an image from a URL or from a local path if it exists.\n",
    "\n",
    "        Args:\n",
    "            url (str): The URL of the image.\n",
    "            path (str): The local path of the image.\n",
    "\n",
    "        Returns:\n",
    "            Image.Image: The loaded image.\n",
    "        \"\"\"\n",
    "        if os.path.exists(path):\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "        else:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code != 200:\n",
    "                raise Exception(\n",
    "                    f\"Failed to download image. Status code: {response.status_code}\"\n",
    "                )\n",
    "            if not response.headers[\"content-type\"].startswith(\"image\"):\n",
    "                raise Exception(\n",
    "                    f\"URL does not point to an image. Content type: {response.headers['content-type']}\"\n",
    "                )\n",
    "            img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "            img.save(path)\n",
    "        img = img.resize((768, 512))\n",
    "        return img\n",
    "\n",
    "    @staticmethod\n",
    "    def random_sublist(lst):\n",
    "        sublist = []\n",
    "        for _ in range(random.randint(1, len(lst))):\n",
    "            item = random.choice(lst)\n",
    "            sublist.append(item)\n",
    "        return sublist\n",
    "\n",
    "    def generate_images(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        image_url: str,\n",
    "        class_name: str,\n",
    "        seed_image_identifier: str,\n",
    "        variations: List[str],\n",
    "        num_images: int = 5,\n",
    "        strength: float = 0.75,\n",
    "        guidance_scale: float = 7.5,\n",
    "        save_path: str = \"output\",\n",
    "        seed_path: str = \"input\",\n",
    "    ) -> List[Image.Image]:\n",
    "        \"\"\"\n",
    "        Generate images based on the provided prompt and variations.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The base prompt for the generation.\n",
    "            image_url (str): The URL of the seed image.\n",
    "            class_name (str): The class of the image (e.g. \"fire\" or \"no_fire\").\n",
    "            seed_image_identifier (str): The identifier of the seed image.\n",
    "            variations (List[str]): The list of variations to apply to the prompt.\n",
    "            num_images (int, optional): The number of images to generate. Defaults to 5.\n",
    "            strength (float, optional): The strength of the transformation. Defaults to 0.75.\n",
    "            guidance_scale (float, optional): The scale of the guidance. Defaults to 7.5.\n",
    "            save_path (str, optional): The path to save the generated images. Defaults to \"output\".\n",
    "            seed_path (str, optional): The path to save the input images. Defaults to \"input\".\n",
    "\n",
    "        Returns:\n",
    "            List[Image.Image]: The list of generated images.\n",
    "        \"\"\"\n",
    "        input_image_path = f\"{seed_path}/{seed_image_identifier}.png\"\n",
    "        init_image = self.get_image_from_url(image_url, input_image_path)\n",
    "        images = []\n",
    "        for i in range(num_images):\n",
    "            variation = variations[i % len(variations)]\n",
    "            final_prompt = f\"{prompt} {variation}\"\n",
    "            image = self.pipeline(\n",
    "                prompt=final_prompt,\n",
    "                image=init_image,\n",
    "                strength=strength,\n",
    "                guidance_scale=guidance_scale,\n",
    "            ).images\n",
    "            output_image_path = os.path.join(\n",
    "                save_path,\n",
    "                f\"{seed_image_identifier}_{'_'.join(variation.split())}_{i}.png\",\n",
    "            )\n",
    "            image[0].save(output_image_path)\n",
    "            images.append(image)\n",
    "        return images\n",
    "\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "base_prompt = \"A close image to this original satellite image with slight change in location\"\n",
    "\n",
    "\n",
    "############### Exercise 3: add text descriptions such as those in the hint #################\n",
    "fire_variations = [\n",
    "    \"day vegetation dull with little saturation\"\n",
    "]\n",
    "no_fire_variations = [\n",
    "    \"day vegetation bright green vibrant saturation\",\n",
    "]\n",
    "###############################################################################################\n",
    "\n",
    "\n",
    "image_urls = {\n",
    "    \"fire\": [\n",
    "        \"https://github.com/intelsoftware/ForestFirePrediction/blob/main/data/real_USGS_NAIP/train/Fire/m_3912105_sw_10_h_20160713.png?raw=true\",\n",
    "        \"https://github.com/intelsoftware/ForestFirePrediction/blob/main/data/real_USGS_NAIP/train/Fire/m_3912113_sw_10_h_20160713.png?raw=true\",\n",
    "        \"https://github.com/intelsoftware/ForestFirePrediction/blob/main/data/real_USGS_NAIP/train/Fire/m_3912114_se_10_h_20160806.png?raw=true\",\n",
    "        \"https://github.com/intelsoftware/ForestFirePrediction/blob/main/data/real_USGS_NAIP/train/Fire/m_3912120_ne_10_h_20160713.png?raw=true\",\n",
    "        \"https://github.com/intelsoftware/ForestFirePrediction/blob/main/data/real_USGS_NAIP/train/Fire/m_4012355_se_10_h_20160713.png?raw=true\",\n",
    "    ],\n",
    "    \"no_fire\": [\n",
    "        \"https://github.com/intelsoftware/ForestFirePrediction/blob/main/data/real_USGS_NAIP/train/NoFire/m_3912045_ne_10_h_20160712.png?raw=true\",\n",
    "        \"https://github.com/intelsoftware/ForestFirePrediction/blob/main/data/real_USGS_NAIP/train/NoFire/m_3912057_sw_10_h_20160711.png?raw=true\",\n",
    "        \"https://github.com/intelsoftware/ForestFirePrediction/blob/main/data/real_USGS_NAIP/train/NoFire/m_3912142_sw_10_h_20160711.png?raw=true\",\n",
    "        \"https://github.com/intelsoftware/ForestFirePrediction/blob/main/data/real_USGS_NAIP/train/NoFire/m_3912343_se_10_h_20160529.png?raw=true\",\n",
    "        \"https://github.com/intelsoftware/ForestFirePrediction/blob/main/data/real_USGS_NAIP/train/NoFire/m_4012241_se_10_h_20160712.png?raw=true\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba0e4f1-6e22-49b4-b90f-17535b8165cd",
   "metadata": {},
   "source": [
    "# IntelÂ® Data Center GPU Max Series Optimized with Intel Extension for PyTorch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6d9a23-b542-41ae-8ae6-8bc55622e5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "timing = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c5cb55-c354-443c-9415-45ceef11b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimize = True\n",
    "\n",
    "model = Img2ImgModel(model_id, device=\"xpu\", optimize=Optimize)\n",
    "num_images = 2\n",
    "gen_img_count = 0\n",
    "\n",
    "try:\n",
    "    trips = 0\n",
    "    start_time = time.time()\n",
    "    for class_name, urls in image_urls.items():\n",
    "        for url in urls:\n",
    "            seed_image_identifier = os.path.basename(url).split(\".\")[0]\n",
    "            input_dir = f\"./input/{class_name}\"\n",
    "            output_dir = f\"./output/{class_name}\"\n",
    "            os.makedirs(input_dir, exist_ok=True)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            variations = (\n",
    "                fire_variations if class_name == \"fire\" else no_fire_variations\n",
    "            )\n",
    "            model.generate_images(\n",
    "                base_prompt,\n",
    "                url,\n",
    "                class_name,\n",
    "                seed_image_identifier,\n",
    "                variations=variations,\n",
    "                save_path=output_dir,\n",
    "                seed_path=input_dir,\n",
    "                num_images=num_images,\n",
    "            )\n",
    "            gen_img_count += num_images\n",
    "            trips += 1\n",
    "            print(\"input image\",trips)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nUser interrupted image generation...\")\n",
    "finally:\n",
    "    timing[\"xpu IPEX Optimized\"] = time.time() - start_time\n",
    "    print(\n",
    "        f\"Complete generating {gen_img_count} images in {'/'.join(output_dir.split('/')[:-1])} in {time.time() - start_time:.2f} seconds.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d05f5a-a38a-4529-84cc-e0ec604eaab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99d50dfc-be7d-4ecc-bddd-c137e96b6b49",
   "metadata": {},
   "source": [
    "# IntelÂ® Data Center GPU Max Series GPU Without IPEX\n",
    "\n",
    "* It is highly recommended that you use the Intel Extension for PyTorch for optimizaing the model and data whenever you are targeting an Intel GPU. Performance degradation can occur if unoptimized model/ data are used.\n",
    "  \n",
    "* Run this when you go to lunch, it takes ~ 17 minutes.  You should get similar results as below, your mileage may vary, depending on how busy Intel Developer Cloud (IDC) is at the time of your workshop.\n",
    "\n",
    "* Compare this code to the fast code above to internalize how model and data shoudl be optimized together with the functions provided\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98a7f51-1bff-452f-b175-d3cdef7254e9",
   "metadata": {},
   "source": [
    "## Notices and Disclaimers\n",
    "\n",
    "Intel technologies may require enabled hardware, software or service activation.\n",
    "\n",
    "No product or component can be absolutely secure. \n",
    "\n",
    "Your costs and results may vary. \n",
    "\n",
    "Â© Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others. \n",
    "\n",
    "### Reference and Guidelines for Models Used in This Notebook\n",
    "\n",
    "### Image-to-Image Models Information\n",
    "\n",
    "#### runwayml/stable-diffusion-v1-5\n",
    "- **Model card:** [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)\n",
    "- **License:** CreativeML OpenRAIL M license\n",
    "- **Reference:**\n",
    "    ```bibtex\n",
    "    @InProceedings{Rombach_2022_CVPR,\n",
    "        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n",
    "        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n",
    "        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n",
    "        month     = {June},\n",
    "        year      = {2022},\n",
    "        pages     = {10684-10695}\n",
    "    }\n",
    "    ```\n",
    "\n",
    "For detailed information on each model's capabilities, limitations, and best practices, please refer to the respective model cards and associated publications linked above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6c3945-3be2-4cc3-a84f-5c0b74589d14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch GPU",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
