{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c26d0ec2-0fc6-4dfd-8f77-f10657db5e8e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stable Diffusion: Accelerated\n",
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"assets/Stable_Diffusion_architecture.png\", width=\"800\">\n",
    "<figcaption align = \"center\"> FIgure 1. Architecture Diagram of Stable Diffusion Components license: MIT Diffusion architecture.png\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "- [wikipedia: Stable Diffusion](https://en.wikipedia.org/wiki/Stable_Diffusion#/media/File:Stable_Diffusion_architecture.png)\n",
    "\n",
    "## Overview\n",
    "\n",
    "\n",
    "Stable Diffusion is a technique that uses a special kind of diffusion model developed by the CompVis group at LMU Munich. This model helps improve images by reducing noise. It has three main parts: a variational autoencoder (VAE) that compresses the image, a U-Net that removes noise, and an optional text encoder. The VAE makes the image smaller and more meaningful, while the U-Net removes noise. The process involves applying Gaussian noise and then removing it. The VAE decoder completes the process by turning the compressed image back into a clear picture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913daccf-ed41-4b1c-b80a-931d8e912ce1",
   "metadata": {},
   "source": [
    "## What can it do?\n",
    "\n",
    "<figure>\n",
    "<img src=\"assets/X-Y_plot_of_algorithmically-generated_AI_art_of_European-style_castle_in_Japan_demonstrating_DDIM_diffusion_steps.png\" width=\"600\">\n",
    "<figcaption align = \"center\"> Figure 2. Image coutesy of Benlisquare at wikipedia: Stable Diffusion </figcaption>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b9fe3d-3e39-46e7-bc8b-fb367a492912",
   "metadata": {},
   "source": [
    "# Stable Diffusion for Synthetic Aerial Photos\n",
    "\n",
    "In this project we downloaded 10 images from US government sources for NAIP/DOQQ images with two classes in mind - photos which will be marred in the future by forest fires, and photos which will remain unphased by forest fires in the next two years (referenving 2016/2017)\n",
    "\n",
    "The code below was taken from Hugging face and optimized by our team!\n",
    "\n",
    "It uses Intel(R) Extension for Pytorch (IPEX) to allow us to target an IntelÂ® Data Center GPU Max Serie gpu on the Intel(r) Developer Cloud Beta\n",
    "\n",
    "The intersting take away here is that the Stable Diffusion model used here is not a single model but rather a pipeline of 4 standalone models and this optimization technique get applied to all models in the pipeline!\n",
    "\n",
    "Significant speedups come from applying IPEX versus not. Try it yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad1d3e5-05f8-4a15-a964-91c8c46e71db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import random\n",
    "import requests\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "class Img2ImgModel:\n",
    "    \"\"\"\n",
    "    This class creates a model for transforming images based on given prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id_or_path: str,\n",
    "        device: str = \"xpu\",\n",
    "        torch_dtype: torch.dtype = torch.float16,\n",
    "        optimize: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the model with the specified parameters.\n",
    "\n",
    "        Args:\n",
    "            model_id_or_path (str): The ID or path of the pre-trained model.\n",
    "            device (str, optional): The device to run the model on. Defaults to \"xpu\".\n",
    "            torch_dtype (torch.dtype, optional): The data type to use for the model. Defaults to torch.float16.\n",
    "            optimize (bool, optional): Whether to optimize the model. Defaults to True.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.pipeline = self._load_pipeline(model_id_or_path, torch_dtype)\n",
    "        if optimize:\n",
    "            start_time = time.time()\n",
    "            print(\"Optimizing the model...\")\n",
    "            self.optimize_pipeline()\n",
    "            print(\n",
    "                \"Optimization completed in {:.2f} seconds.\".format(\n",
    "                    time.time() - start_time\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def _load_pipeline(\n",
    "        self, model_id_or_path: str, torch_dtype: torch.dtype\n",
    "    ) -> StableDiffusionImg2ImgPipeline:\n",
    "        \"\"\"\n",
    "        Load the pipeline for the model.\n",
    "\n",
    "        Args:\n",
    "            model_id_or_path (str): The ID or path of the pre-trained model.\n",
    "            torch_dtype (torch.dtype): The data type to use for the model.\n",
    "\n",
    "        Returns:\n",
    "            StableDiffusionImg2ImgPipeline: The loaded pipeline.\n",
    "        \"\"\"\n",
    "        print(\"Loading the model...\")\n",
    "        pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "            model_id_or_path, torch_dtype=torch_dtype\n",
    "        )\n",
    "        pipeline = pipeline.to(self.device)\n",
    "        print(\"Model loaded.\")\n",
    "        return pipeline\n",
    "\n",
    "    def _optimize_pipeline(\n",
    "        self, pipeline: StableDiffusionImg2ImgPipeline\n",
    "    ) -> StableDiffusionImg2ImgPipeline:\n",
    "        \"\"\"\n",
    "        Optimize the pipeline of the model.\n",
    "\n",
    "        Args:\n",
    "            pipeline (StableDiffusionImg2ImgPipeline): The pipeline to optimize.\n",
    "\n",
    "        Returns:\n",
    "            StableDiffusionImg2ImgPipeline: The optimized pipeline.\n",
    "        \"\"\"\n",
    "        for attr in dir(pipeline):\n",
    "            if isinstance(getattr(pipeline, attr), nn.Module):\n",
    "                setattr(\n",
    "                    pipeline,\n",
    "                    attr,\n",
    "                    ipex.optimize(\n",
    "                        getattr(pipeline, attr).eval(),\n",
    "                        dtype=pipeline.text_encoder.dtype,\n",
    "                        inplace=True,\n",
    "                    ),\n",
    "                )\n",
    "        return pipeline\n",
    "\n",
    "    def optimize_pipeline(self) -> None:\n",
    "        \"\"\"\n",
    "        Optimize the pipeline of the model.\n",
    "        \"\"\"\n",
    "        self.pipeline = self._optimize_pipeline(self.pipeline)\n",
    "\n",
    "    def get_image_from_url(self, url: str, path: str) -> Image.Image:\n",
    "        \"\"\"\n",
    "        Get an image from a URL or from a local path if it exists.\n",
    "\n",
    "        Args:\n",
    "            url (str): The URL of the image.\n",
    "            path (str): The local path of the image.\n",
    "\n",
    "        Returns:\n",
    "            Image.Image: The loaded image.\n",
    "        \"\"\"\n",
    "        if os.path.exists(path):\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "        else:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code != 200:\n",
    "                raise Exception(\n",
    "                    f\"Failed to download image. Status code: {response.status_code}\"\n",
    "                )\n",
    "            if not response.headers[\"content-type\"].startswith(\"image\"):\n",
    "                raise Exception(\n",
    "                    f\"URL does not point to an image. Content type: {response.headers['content-type']}\"\n",
    "                )\n",
    "            img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "            img.save(path)\n",
    "        img = img.resize((768, 512))\n",
    "        return img\n",
    "\n",
    "    @staticmethod\n",
    "    def random_sublist(lst):\n",
    "        sublist = []\n",
    "        for _ in range(random.randint(1, len(lst))):\n",
    "            item = random.choice(lst)\n",
    "            sublist.append(item)\n",
    "        return sublist\n",
    "\n",
    "    def generate_images(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        image_url: str,\n",
    "        class_name: str,\n",
    "        seed_image_identifier: str,\n",
    "        variations: List[str],\n",
    "        num_images: int = 5,\n",
    "        strength: float = 0.75,\n",
    "        guidance_scale: float = 7.5,\n",
    "        save_path: str = \"output\",\n",
    "        seed_path: str = \"intput\",\n",
    "    ) -> List[Image.Image]:\n",
    "        \"\"\"\n",
    "        Generate images based on the provided prompt and variations.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The base prompt for the generation.\n",
    "            image_url (str): The URL of the seed image.\n",
    "            class_name (str): The class of the image (e.g. \"fire\" or \"no_fire\").\n",
    "            seed_image_identifier (str): The identifier of the seed image.\n",
    "            variations (List[str]): The list of variations to apply to the prompt.\n",
    "            num_images (int, optional): The number of images to generate. Defaults to 5.\n",
    "            strength (float, optional): The strength of the transformation. Defaults to 0.75.\n",
    "            guidance_scale (float, optional): The scale of the guidance. Defaults to 7.5.\n",
    "            save_path (str, optional): The path to save the generated images. Defaults to \"output\".\n",
    "            seed_path (str, optional): The path to save the input images. Defaults to \"input\".\n",
    "\n",
    "        Returns:\n",
    "            List[Image.Image]: The list of generated images.\n",
    "        \"\"\"\n",
    "        input_image_path = f\"{seed_path}/{seed_image_identifier}.png\"\n",
    "        init_image = self.get_image_from_url(image_url, input_image_path)\n",
    "        images = []\n",
    "        for i in range(num_images):\n",
    "            variation = variations[i % len(variations)]\n",
    "            final_prompt = f\"{prompt} {variation}\"\n",
    "            image = self.pipeline(\n",
    "                prompt=final_prompt,\n",
    "                image=init_image,\n",
    "                strength=strength,\n",
    "                guidance_scale=guidance_scale,\n",
    "            ).images\n",
    "            output_image_path = os.path.join(\n",
    "                save_path,\n",
    "                f\"{seed_image_identifier}_{'_'.join(variation.split())}_{i}.png\",\n",
    "            )\n",
    "            image[0].save(output_image_path)\n",
    "            images.append(image)\n",
    "        return images\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "    base_prompt = \"A close image to this original satellite image with slight change in location\"\n",
    "    fire_variations = [\n",
    "        \"vegetation dark green and brown low saturation image mountainous slopes early morning clear skies\",\n",
    "        \"vegetation dark green and brown low saturation image mountainous slopes late afternoon clear skies\",\n",
    "        \"vegetation dark green and brown low saturation image mountainous slopes mid-day no clouds\",\n",
    "        \"vegetation dark green and brown low saturation image mountainous slopes mid-day minimal light sirius cloud cover\",\n",
    "        \"vegetation dark green and brown low saturation image mountainous slopes smoky conditions\",\n",
    "        \"vegetation dark green and brown low saturation image mountainous slopes late afternoon minimal light sirius cloud cover\",\n",
    "    ]\n",
    "    no_fire_variations = [\n",
    "        \"early morning with clear skies vegetation bright green vibrant saturation\",\n",
    "        \"no signs of fire vegetation bright green vibrant saturation\",\n",
    "        \"day vegetation bright green vibrant saturation\",\n",
    "        \"late afternoon with clear skies vegetation bright green vibrant saturation\",\n",
    "        \"mid-day with clear skies vegetation bright green vibrant saturation\",\n",
    "        \"with dense vegetation vegetation bright green vibrant saturation\",\n",
    "        \"with sparse vegetation vegetation bright green vibrant saturation\",\n",
    "    ]\n",
    "\n",
    "    image_urls = {\n",
    "        \"fire\": [\n",
    "            \"https://github.com/rahulunair/ForestFirePrediction/blob/main/data/output/train/Fire/m_3912105_sw_10_h_20160713.png?raw=true\",\n",
    "            \"https://github.com/rahulunair/ForestFirePrediction/blob/main/data/output/train/Fire/m_3912113_sw_10_h_20160713.png?raw=true\",\n",
    "            \"https://github.com/rahulunair/ForestFirePrediction/blob/main/data/output/train/Fire/m_3912114_se_10_h_20160806.png?raw=true\",\n",
    "            \"https://github.com/rahulunair/ForestFirePrediction/blob/main/data/output/train/Fire/m_3912120_ne_10_h_20160713.png?raw=true\",\n",
    "            \"https://github.com/rahulunair/ForestFirePrediction/blob/main/data/output/train/Fire/m_4012355_se_10_h_20160713.png?raw=true\",\n",
    "        ],\n",
    "        \"no_fire\": [\n",
    "            \"https://github.com/rahulunair/ForestFirePrediction/blob/main/data/output/train/NoFire/m_3912045_ne_10_h_20160712.png?raw=true\",\n",
    "            \"https://github.com/rahulunair/ForestFirePrediction/blob/main/data/output/train/NoFire/m_3912057_sw_10_h_20160711.png?raw=true\",\n",
    "            \"https://github.com/rahulunair/ForestFirePrediction/blob/main/data/output/train/NoFire/m_3912142_sw_10_h_20160711.png?raw=true\",\n",
    "            \"https://github.com/rahulunair/ForestFirePrediction/blob/main/data/output/train/NoFire/m_3912343_se_10_h_20160529.png?raw=true\",\n",
    "            \"https://github.com/rahulunair/ForestFirePrediction/blob/main/data/output/train/NoFire/m_4012241_se_10_h_20160712.png?raw=true\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    model = Img2ImgModel(model_id, device=\"xpu\")\n",
    "    num_images = 5\n",
    "    gen_img_count = 0\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        for class_name, urls in image_urls.items():\n",
    "            for url in urls:\n",
    "                seed_image_identifier = os.path.basename(url).split(\".\")[0]\n",
    "                input_dir = f\"./input/{class_name}\"\n",
    "                output_dir = f\"./output/{class_name}\"\n",
    "                os.makedirs(input_dir, exist_ok=True)\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                variations = (\n",
    "                    fire_variations if class_name == \"fire\" else no_fire_variations\n",
    "                )\n",
    "                model.generate_images(\n",
    "                    base_prompt,\n",
    "                    url,\n",
    "                    class_name,\n",
    "                    seed_image_identifier,\n",
    "                    variations=variations,\n",
    "                    save_path=output_dir,\n",
    "                    seed_path=input_dir,\n",
    "                    num_images=num_images,\n",
    "                )\n",
    "                gen_img_count += num_images\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nUser interrupted image generation...\")\n",
    "    finally:\n",
    "        print(\n",
    "            f\"Complete generating {gen_img_count} images in {'/'.join(output_dir.split('/')[:-1])} in {time.time() - start_time:.2f} seconds.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c8d187-9515-4005-816a-3c2de286a573",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
